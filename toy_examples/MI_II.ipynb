{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     \n",
    "\"\"\"\n",
    "=================================================\n",
    "Imputing missing values using multiple imputation\n",
    "=================================================\n",
    "The :class:`sklearn.impute.IterativeImputer` is able to generate multiple\n",
    "imputations of the same incomplete dataset. We can then learn a regression\n",
    "or classification model on different imputations of the same dataset.\n",
    "This allows us to quantify how much the regressor varies due to the uncertainty\n",
    "inherent in missing values. Specifically, we can estimate the variance of\n",
    "predictions, or of the model coefficients, using multiple imputation.\n",
    "Setting :class:`sklearn.impute.IterativeImputer`'s `sample_posterior=True` will\n",
    "randomly draw values to fill each missing value from the Gaussian posterior of\n",
    ":class:`sklearn.linear_model.BayesianRidge` predictions. If each\n",
    ":class:`sklearn.impute.IterativeImputer` uses a different `random_state`, this\n",
    "results in multiple imputations, each of which can be used to train a\n",
    "predictive model.\n",
    "One final model is obtained by combining the estimates of each model with\n",
    "Rubin's pooling rules, which are as follows. The overall point estimate after\n",
    "multiple imputation (denoted by Qbar) is the average of all the m point\n",
    "estimates. The variance of the overall point estimate is a combination of\n",
    "so-called within imputation variance (Ubar) and between imputation\n",
    "variance (B). Ubar is the average of the m variances of the m point estimates.\n",
    "Both Qbar and Ubar are corrected with a factor 1 / m to account for sampling\n",
    "variance. The between imputation variance (B) is the sum of the squared\n",
    "difference between Qbar and the m point estimates, corrected with a factor\n",
    "1 / (m – 1). Then, the total variance (T) of the MI overall point estimate is\n",
    "Ubar + B + B/m.\n",
    "These rules assume that the parameters of interest are normally distributed\n",
    "which is the case with, for example, estimates of the mean and regression\n",
    "coefficients. Other parameters, such as correlation coefficients need\n",
    "transformation to suit the assumption of normality. If it is not possible to\n",
    "approximate a normal distribution, it is better to use robust summary measures\n",
    "such as medians or ranges instead of using Rubin's pooling rules. This applies\n",
    "to an estimate like explained variance.\n",
    "In this Example we show how to use :class:`sklearn.impute.IterativeImputer`\n",
    "to perform multiple imputation. In Example 1 we show the effect of Rubin’s\n",
    "pooling rules on the variance of regression estimates. Due to the between\n",
    "imputation variance, the standard errors of all regression coefficients are\n",
    "larger with multiple imputation than with single imputation.\n",
    "In Example 2 we show how to set up a prediction model using multiple\n",
    "imputation. We combine the predictions from multiple imputations and show that\n",
    "this results in better regression performance.\n",
    "\"\"\"\n",
    "print(__doc__)\n",
    "\n",
    "# Authors: Rianne Schouten <r.m.schouten@uu.nl>\n",
    "#          Sergey Feldman <sergeyfeldman@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import special\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "\n",
    "def ampute(X, missing_rate=0.75, strategy=\"MCAR\"):\n",
    "    \"\"\"Insert missing data into X.\n",
    "    Ampute is the inverse of impute and serves at simulating a dataset with\n",
    "    missing data. Two strategies are implemented to remove data: (1) missing\n",
    "    completely at random (MCAR) and (2) not missing completely at random\n",
    "    (NMCAR).\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape (n_samples, n_features)\n",
    "        The data without missing samples.\n",
    "    missing_rate : float, default=0.75\n",
    "        The amount of missing data in ``X``.\n",
    "    strategy : {'MCAR', 'NMCAR'}, default='MCAR'\n",
    "        The strategy for removing data. Must be 'MCAR' or 'NMCAR'.\n",
    "        If MCAR, each sample will have a random missing feature.\n",
    "        If NMCAR, each sample will have a random missing feature. However,\n",
    "        values that are farther from the feature mean are more likely to be\n",
    "        made missing.\n",
    "    Returns\n",
    "    -------\n",
    "    X_out : ndarray, shape (n_samples, n_features)\n",
    "        Output data with missing entries.\n",
    "    \"\"\"\n",
    "    X_out = X.copy()\n",
    "    n_drop_per_feature = int(missing_rate * X.shape[0])\n",
    "    for x in np.transpose(X_out):\n",
    "        if strategy == \"MCAR\":\n",
    "            prob = None\n",
    "        elif strategy == \"NMCAR\":\n",
    "            weights = special.expit(np.abs(x - x.mean()))\n",
    "            prob = weights / weights.sum()\n",
    "        drop_idx = np.random.choice(\n",
    "            X.shape[0], p=prob, replace=False, size=n_drop_per_feature\n",
    "        )\n",
    "        x[drop_idx] = np.nan\n",
    "    return X_out\n",
    "\n",
    "\n",
    "def variance_model_coef(y_true, y_pred, X):\n",
    "    \"\"\"Calculates variance of the coefficients.\"\"\"\n",
    "    residual_sum_squares = np.sum((y_true - y_pred) ** 2)\n",
    "    residual_mean_square = residual_sum_squares / (len(y_true) - 2)\n",
    "    return residual_mean_square / (X ** 2).sum(axis=0)\n",
    "\n",
    "\n",
    "def rubins_pooling_rules(m_coefs, m_vars_coefs):\n",
    "    \"\"\"Applies Rubin's pooling rules.\n",
    "    The final weights is defined as the mean of the weights across the imputed\n",
    "    datasets while the total variance is defined as the combination of the mean\n",
    "    of the variance of the weights and the variance of the coefficients.\n",
    "    Parameters\n",
    "    ----------\n",
    "    m_coefs : ndarray, shape (n_imputations, n_features)\n",
    "        The weights of the model fitted on each imputed dataset.\n",
    "    m_vars_coefs : ndarray, shape (n_imputations, n_features)\n",
    "        An estimate of the variance of the weights on each imputed dataset.\n",
    "    Returns\n",
    "    -------\n",
    "    mean_coefs : ndarray, shape (n_features,)\n",
    "        The mean coefficients computed across the imputed datasets.\n",
    "    total_var_coefs : ndarray, shape (n_features,)\n",
    "        An estimate of the total variation of the weights across the imputed\n",
    "        datasets.\n",
    "    \"\"\"\n",
    "    mean_coefs = np.mean(m_coefs, axis=0)\n",
    "    mean_vars_coefs = np.mean(m_vars_coefs, axis=0)\n",
    "    vars_coefs = np.var(m_coefs, axis=0, ddof=1)\n",
    "    total_var_coefs = mean_vars_coefs + (1 + 1 / vars_coefs.shape[0]) * vars_coefs\n",
    "    return mean_coefs, total_var_coefs\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# EXAMPLE 1. COMPARE STATISTICAL ESTIMATES AND THEIR VARIANCE USING MULTIPLE\n",
    "# IMPUTATION IN A LINEAR REGRESSION MODEL.\n",
    "\n",
    "MAX_ITER = 20  # number of iterations in IterativeImputer\n",
    "ESTIMATOR = Pipeline(\n",
    "    steps=[(\"scaler\", StandardScaler()), (\"regressor\", BayesianRidge())]\n",
    ")\n",
    "\n",
    "\n",
    "def get_results_full_dataset(X, y):\n",
    "    y_predict = ESTIMATOR.fit(X, y).predict(X)\n",
    "    # return coefficients and variance of coefficients\n",
    "    return (\n",
    "        ESTIMATOR.named_steps[\"regressor\"].coef_,\n",
    "        variance_model_coef(y, y_predict, X),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_results_chained_imputation(X_ampute, y, random_state=0, impute_with_y=False):\n",
    "    # Impute incomplete data with IterativeImputer using single imputation\n",
    "    # We perform MAX_ITER imputations and only use the last imputation.\n",
    "    imputer = IterativeImputer(\n",
    "        max_iter=MAX_ITER, sample_posterior=True, random_state=random_state\n",
    "    )\n",
    "    if impute_with_y:\n",
    "        Xy = np.column_stack((X_ampute, y))\n",
    "        # impute Xy, but exclude last column for subsequent regression\n",
    "        X_imputed = imputer.fit_transform(Xy)[:, :-1]\n",
    "    else:\n",
    "        X_imputed = imputer.fit_transform(X_ampute)\n",
    "\n",
    "    # Perform linear regression on chained single imputed data\n",
    "    # Estimate beta estimates and their variances\n",
    "    y_predict = ESTIMATOR.fit(X_imputed, y).predict(X_imputed)\n",
    "\n",
    "    # Save the beta estimates, the variance of these estimates\n",
    "    return (\n",
    "        ESTIMATOR.named_steps[\"regressor\"].coef_,\n",
    "        variance_model_coef(y, y_predict, X_imputed),\n",
    "    )\n",
    "\n",
    "\n",
    "def coef_var_mice_imputation(X_ampute, y, n_imputations=5, impute_with_y=False):\n",
    "    # Train a model on each of the `m` imputed datasets\n",
    "    # Estimate the estimates for each model/dataset\n",
    "    m_coefs = []\n",
    "    m_vars = []\n",
    "    for i in range(n_imputations):\n",
    "        m_coef, m_var = get_results_chained_imputation(\n",
    "            X_ampute, y, random_state=i, impute_with_y=impute_with_y\n",
    "        )\n",
    "        m_coefs.append(m_coef)\n",
    "        m_vars.append(m_var)\n",
    "\n",
    "    m_coefs = np.array(m_coefs)\n",
    "    m_vars = np.array(m_vars)\n",
    "    # Calculate the end estimates by applying Rubin's rules.\n",
    "    return rubins_pooling_rules(m_coefs, m_vars)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Now let's run all these imputation procedures.\n",
    "# We use the Boston dataset and analyze the outcomes of the beta coefficients\n",
    "# and their standard errors. We standardize the data before running the\n",
    "# procedure to be able to compare the coefficients. We run the procedure for\n",
    "# MCAR missingness only.\n",
    "#\n",
    "# Note: the original multiple imputation procedure as developed under the name\n",
    "# MICE includes all variables in the imputation process; including the output\n",
    "# variable. The reason to do this is that the imputation model should at least\n",
    "# contain the analysis model to result in unbiased estimates. In this function,\n",
    "# we will also include `y` in the imputation process.\n",
    "\n",
    "\n",
    "# Loading the data\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "# Start the procedure\n",
    "print(\"Executing Example 1 MCAR Missingness...\")\n",
    "\n",
    "# First, make the data incomplete with a MCAR mechanism.\n",
    "X_ampute = ampute(X, strategy=\"MCAR\")\n",
    "\n",
    "# Second, run all the imputation procedures as described above.\n",
    "full_coefs, full_vars = get_results_full_dataset(X, y)\n",
    "chained_coefs, chained_vars = get_results_chained_imputation(X_ampute, y)\n",
    "mice_coefs, mice_vars = coef_var_mice_imputation(X_ampute, y)\n",
    "mice_y_coefs, mice_y_vars = coef_var_mice_imputation(X_ampute, y, impute_with_y=True)\n",
    "\n",
    "# Combine the results from the four imputation procedures.\n",
    "coefs = [full_coefs, chained_coefs, mice_coefs, mice_y_coefs]\n",
    "standard_errors = [\n",
    "    1.96 * np.sqrt(v) for v in (full_vars, chained_vars, mice_vars, mice_y_vars)\n",
    "]\n",
    "\n",
    "# And plot the results\n",
    "n_situations = 4\n",
    "n = np.arange(n_situations)\n",
    "n_labels = [\n",
    "    \"Full Data\",\n",
    "    \"IterativeImputer\",\n",
    "    \"Mice Imputer\",\n",
    "    \"Mice Imputer with y\",\n",
    "]\n",
    "colors = [\"r\", \"orange\", \"b\", \"purple\"]\n",
    "width = 0.3\n",
    "plt.figure(figsize=(6, 8))\n",
    "\n",
    "plt1 = plt.subplot(211)\n",
    "for j in n:\n",
    "    plt1.bar(\n",
    "        np.arange(len(coefs[j])) + (3 * j * (width / n_situations)),\n",
    "        coefs[j],\n",
    "        width=width,\n",
    "        color=colors[j],\n",
    "    )\n",
    "plt.legend(n_labels)\n",
    "\n",
    "plt2 = plt.subplot(212)\n",
    "for j in n:\n",
    "    plt2.bar(\n",
    "        np.arange(len(standard_errors[j])) + (3 * j * (width / n_situations)),\n",
    "        standard_errors[j],\n",
    "        width=width,\n",
    "        color=colors[j],\n",
    "    )\n",
    "\n",
    "plt1.set_title(\"MCAR Missingness\")\n",
    "plt1.set_ylabel(\"Beta Coefficients\")\n",
    "plt2.set_ylabel(\"Standard Errors\")\n",
    "plt1.set_xlabel(\"Features\")\n",
    "plt2.set_xlabel(\"Features\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# EXAMPLE 2. SHOW MULTIPLE IMPUTATION IN A PREDICTION CONTEXT.\n",
    "\n",
    "\n",
    "# In this example, we show how to apply multiple imputation in a train/test\n",
    "# situation. To do so, you average the predictions of the models fit on every\n",
    "# imputed dataset, and then calculate the evaluation metric.\n",
    "\n",
    "N_SIM = 3  # number of simulations in Example 2\n",
    "ESTIMATOR_IMPUTER = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", IterativeImputer()),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"regressor\", BayesianRidge()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Use the IterativeImputer as a single imputation procedure.\n",
    "def get_mse_single_imputation(X_train, X_test, y_train, y_test, random_state=0):\n",
    "    ESTIMATOR_IMPUTER.set_params(\n",
    "        imputer__max_iter=MAX_ITER,\n",
    "        imputer__sample_posterior=True,\n",
    "        imputer__random_state=random_state,\n",
    "    )\n",
    "    y_predict = ESTIMATOR_IMPUTER.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "    return mse(y_test, y_predict), y_predict\n",
    "\n",
    "\n",
    "# We average the predictions of the m datasets.\n",
    "def get_mse_multiple_imputation_approach(X_train, X_test, y_train, y_test):\n",
    "    num_mice_runs = 5\n",
    "    multiple_predictions = []\n",
    "    for i in range(num_mice_runs):\n",
    "        _, y_predict = get_mse_single_imputation(\n",
    "            X_train, X_test, y_train, y_test, random_state=i\n",
    "        )\n",
    "        multiple_predictions.append(y_predict)\n",
    "\n",
    "    # Average the predictions over the m loops\n",
    "    # Then calculate the error metric.\n",
    "    predictions_average = np.mean(multiple_predictions, axis=0)\n",
    "    mse_multiple = mse(y_test, predictions_average)\n",
    "\n",
    "    return mse_multiple, predictions_average\n",
    "\n",
    "\n",
    "def perform_simulation(X, y, X_ampute, n_simulation=10):\n",
    "    # Start a simulation process that executes the process n_simulation times.\n",
    "    outcome = []\n",
    "    for simulation_idx in range(n_simulation):\n",
    "        # First, split the data in train and test dataset.\n",
    "        (\n",
    "            X_train,\n",
    "            X_test,\n",
    "            X_ampute_train,\n",
    "            X_ampute_test,\n",
    "            y_train,\n",
    "            y_test,\n",
    "        ) = train_test_split(X, X_ampute, y, random_state=simulation_idx)\n",
    "\n",
    "        # Second, perform the imputation procedures and calculation of the\n",
    "        # error metric for every one of the  situations.\n",
    "        # Apply the regression model on the full dataset as a way of comparison\n",
    "        y_predict = ESTIMATOR.fit(X_train, y_train).predict(X_test)\n",
    "        mse_full = mse(y_test, y_predict)\n",
    "        mse_single, _ = get_mse_single_imputation(\n",
    "            X_ampute_train, X_ampute_test, y_train, y_test\n",
    "        )\n",
    "        mse_multiple, _ = get_mse_multiple_imputation_approach(\n",
    "            X_ampute_train, X_ampute_test, y_train, y_test\n",
    "        )\n",
    "\n",
    "        # Save the outcome of every simulation round\n",
    "        outcome.append((mse_full, mse_single, mse_multiple))\n",
    "\n",
    "    # Return the mean and standard deviation of the n_simulation outcome values\n",
    "    return np.mean(outcome, axis=0), np.std(outcome, axis=0)\n",
    "\n",
    "\n",
    "# Execute the simulation\n",
    "print(\"Executing Example 2 MCAR Missingness...\")\n",
    "\n",
    "# Perform the simulation\n",
    "mse_means, mse_std = perform_simulation(X, y, X_ampute, n_simulation=N_SIM)\n",
    "\n",
    "# Plot results\n",
    "n_situations = 3\n",
    "n = np.arange(n_situations)\n",
    "n_labels = [\"Full Data\", \"Single Imputation\", \"Multiple Imputations\"]\n",
    "colors = [\"r\", \"orange\", \"green\"]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax1 = plt.subplot(111)\n",
    "for j in n:\n",
    "    ax1.barh(\n",
    "        j,\n",
    "        mse_means[j],\n",
    "        xerr=mse_std[j],\n",
    "        color=colors[j],\n",
    "        alpha=0.6,\n",
    "        align=\"center\",\n",
    "    )\n",
    "\n",
    "ax1.set_title(\"MCAR Missingness\")\n",
    "ax1.set_yticks(n)\n",
    "ax1.set_xlabel(\"Mean Squared Error\")\n",
    "ax1.invert_yaxis()\n",
    "ax1.set_yticklabels(n_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33d27e0b0f2f7de18c97883b105d2a38655729a870f4d8c8ca5d2b8ade76ede4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
